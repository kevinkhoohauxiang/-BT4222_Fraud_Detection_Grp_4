{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "def header(text):\n",
    "    print(f\"\\033[94m\\033[1m\", text, f\"\\033[0m\")\n",
    "\n",
    "def num_nulls(df, perc=False) -> dict:\n",
    "    nulls_dict = {}\n",
    "    for col in df.columns:\n",
    "        num_nulls = int(df[col].isnull().sum())\n",
    "        if perc == True:\n",
    "            nulls_dict[col] = (num_nulls / len(df[col])) * 100\n",
    "        else:\n",
    "            nulls_dict[col] = num_nulls\n",
    "    return nulls_dict\n",
    "\n",
    "def split_features(df, split_by):\n",
    "    others = df.drop(columns=split_by)\n",
    "    split = df.loc[:, split_by]\n",
    "    return split, others\n",
    "\n",
    "def extract_df(df):\n",
    "    num_cols = list(df.select_dtypes('number').columns)\n",
    "    cat_cols = [j for j in df.columns if j not in num_cols]\n",
    "    numeric, cat = split_features(df, split_by=num_cols)\n",
    "    return numeric, cat\n",
    "\n",
    "def preprocess_data(\n",
    "        df,\n",
    "        target:str,\n",
    "        remove_nulls_threshold:int,\n",
    "        remove_nulls=False,\n",
    "        interpolate=False,\n",
    "        standardise=False,\n",
    "        ) -> pd.DataFrame:\n",
    "    \n",
    "    # split independent & target:\n",
    "    y, Xs = split_features(df, target)\n",
    "    \n",
    "    # number of independent features:\n",
    "    k = len(Xs.columns)\n",
    "    \n",
    "    # work on Y feature:\n",
    "    if y.isnull().sum() > 0:\n",
    "        y.interpolate(limit_area='inside')\n",
    "        print('Target Feature has missing values being interpolated.')\n",
    "    elif y.unique().sum() != 1:\n",
    "        print('Target Feature is not binary.')\n",
    "    else:\n",
    "        y = pd.to_numeric(y)\n",
    "\n",
    "    # remove any columns with constant values\n",
    "    Xs = Xs[Xs.columns[Xs.nunique() > 1]]\n",
    "    header('Number of Columns with constant values dropped:') \n",
    "    print(f'{k - len(Xs.columns)}')\n",
    "\n",
    "    # replace any non-NaN missing values with NaN\n",
    "    Xs = Xs.replace([float('inf'), float('-inf'), None, 'NULL', 'Null', 'null'], float('nan'))\n",
    "    \n",
    "    if interpolate is True:\n",
    "        Xs = Xs.interpolate(limit_area='inside')\n",
    "        header(f'Features with missing values interpolated.')\n",
    "        \n",
    "    # remove any columns that have > x null values\n",
    "    if remove_nulls is True:\n",
    "        null_dict = num_nulls(Xs, perc=True)\n",
    "        nullcols = [i for i in null_dict.keys() if null_dict[i] >= remove_nulls_threshold]\n",
    "        Xs = Xs.loc[:, [c for c in Xs.columns if c not in nullcols]]\n",
    "        header('Number of Columns with > 50% NaN droped:')\n",
    "        print(f'{len(nullcols)}')\n",
    "\n",
    "    # standardise numerical columns\n",
    "    if standardise is True:\n",
    "        z = StandardScaler()\n",
    "        num, cat = extract_df(Xs)\n",
    "        for j in num.columns:\n",
    "            if j == 'TransactionID':\n",
    "                continue\n",
    "            else:\n",
    "                Xs[j] = z.fit_transform(Xs[j].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    # encode any bool columns and any strings labelled T/F to 1/0\n",
    "    Xs[Xs.columns[Xs.dtypes == 'bool']] += 0\n",
    "    for x in Xs.columns:\n",
    "        if ('T' in set(Xs[x])) or ('F' in set(Xs[x])):\n",
    "            Xs[x] = Xs[x].replace('T', 1).replace('F', 0)\n",
    "\n",
    "    # rejoin Xs and Y features\n",
    "    newdf = pd.concat([Xs, y], axis=1)\n",
    "\n",
    "    return newdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_train = pd.read_csv('train_transaction.csv')\n",
    "cleaned_transaction_train = preprocess_data(df=transaction_train,\n",
    "                                            target='isFraud',\n",
    "                                            remove_nulls=True,\n",
    "                                            remove_nulls_threshold=50,\n",
    "                                            interpolate=False,\n",
    "                                            standardise=True     \n",
    "                                    )\n",
    "\n",
    "display(cleaned_transaction_train)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
